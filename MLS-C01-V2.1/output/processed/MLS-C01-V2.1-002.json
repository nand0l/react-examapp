{
    "ExamID": "MLS-C01-V2.1",
    "QuestionID": "002",
    "Answer": "B",
    "CorrectAnswers": "B. Push the data from Microsoft SQL Server to Amazon S3 using an AWS Data Pipeline and provide the S3 location within the notebook.",
    "Explanation": "Option B is the best approach for this scenario. Amazon SageMaker works most efficiently with data stored in Amazon S3. By moving the data from Amazon RDS to S3 using AWS Data Pipeline, the Machine Learning Specialist can easily access and use the data within SageMaker notebooks. This approach also provides a scalable and cost-effective storage solution for large datasets, which is important when moving from a proof of concept to a full implementation. Additionally, using S3 allows for better integration with other AWS services and provides a centralized location for data that can be easily accessed by SageMaker for training models.",
    "PossibleAnswers": [
        "A. Write a direct connection to the SQL database within the notebook and pull data in",
        "B. Push the data from Microsoft SQL Server to Amazon S3 using an AWS Data Pipeline and provide the S3 location within the notebook.",
        "C. Move the data to Amazon DynamoDB and set up a connection to DynamoDB within the notebook to pull data in",
        "D. Move the data to Amazon ElastiCache using AWS DMS and set up a connection within the notebook to pull data in for fast access."
    ],
    "QuestionText": "A Machine Learning Specialist has completed a proof of concept for a company using a small data sample and now the Specialist is ready to implement an end-to-end solution in AWS using Amazon SageMaker The historical training data is stored in Amazon RDS\nWhich approach should the Specialist use for training a model using that data?"
}